# Proximal Policy Optimization

## Постановка задачи
С помощью алгоритма Proximal Policy Optimization нужно обучить политику, решающую две подзадачи

1. Подъем маятника из нижнего положения в верхнее с последующей стабилизацией 
2. Постановка конца маятника в соответствии с заданным положением в мировых координатах - далее **таргет**

## Основные используемые идеи
С имплементацией PPO проблем нет, основная задача состоит в разработке функции награды. Она должна влиять на:  

- размещение конца маятника около таргета  
- раскручивание в нижний точках(приложение силы и достижение нужной скорости для подъема)  
- стабилизация при раскручивании в верхней точке

Следующий момент состоит в том, как можно определить состояние.

Первый вариант:

- положение тележки - $x$ м
- угол маятника с вертикалью - $\theta$ радиан
- скорость тележки - $\frac{dx}{dt}$ м/с
- угловая скорость конца маятника - $\frac{d\theta}{dt}$ рад/с
- положение таргета - $target$ м
  
Второй вариант:

- положение тележки относительно таргета - $х - target$ м
- угол маятника с вертикалью - $\theta$ радиан
- угловая скорость тележки - $\frac{dx}{dt}$ м/с
- скорость конца маятника - $\frac{d\theta}{dt}$ рад/с
  
Для этой и других моделей установим максимальное время жизни. Для агентов, чья задача ограничивается размещением около таргета, добавим допустимые углы маятника с вертикалью в 0.2 радиана. Агенту не нужно много времени для раскачивания и стабилизации, и нет смысла исследовать мир вдоль оси x.

### Функция награды
Начнем с первого пункта. В похожих окружениях агент получает награду, если удерживает объект в пределах 0.2 радиан(угол с вертикалью). Используем самое простое - косинус. Для размещения около таргета используем штраф или поощрение - некоторая функция от нормы расстояния от центра текущей системы координат(таргета или начала координат).
```python
reward = np.cos(theta) - (x - target)**2
```

```python
reward = np.cos(theta) - np.abs(x - target)
```

```python
reward = np.cos(theta) - np.exp(1 - (x - target)**2)
```

```python
reward = np.cos(theta) - np.exp(1 - np.abs(x - target))
```

Для чего начали с этой модели? Для того чтобы:

-  определить подходящее поведение награды в пределах 0.2 радиан
-  выбрать L1 или L2 норму
-  сравнить поведение модели  на двух вариантах определения состояния
-  настроить гиперпараметры (количество эпох)

Можно также заметить что агент не стремиться попасть прямо в таргет, а останавливается около него. Можно добавить в окрестности таргета побольше награды.

Второй пункт немного сложнее. Мы должны поощрять скорость по направлению вверх, но тогда есть субоптимальная стратегия -  это бесконечно крутиться. C последним будем бороться тем, что при прохождении нижнего части круга суммарная награда дает неположительную величину. Проще всего будет наказывать за движение вниз, а на движение вверх не реагировать. Также чтобы агент начал пробовать двигаться добавим награду за работу мотора.
```python
reward = min(-theta**2 + 0.1 * dtheta**2 + 0.001 * min(a**2, 1.0) - 10.0, 0.0) + np.exp(-(x - target)**2)
```
В третьем пункте будем наказывать за падение в верхней части и поощрять за возвышение
```python
reward = np.cos(theta) * (1 - dtheta**2) - max(dtheta * theta, 0.0)
```

Итоговый вариант выглядит так:

## Эксперименты

В начале обучались агенты для уравновешивания маятника и устанавливания конца маятника в начало системы координат. Вариант с пространством наблюдений размерности 5 давал стабильнее обучение чем с пространством наблюдений размерности 5, но сходился дольше. Количество эпох из этих экспериментов нужно было брать небольшое - 8-32. Награда с нормой L1 прастет быстрее штрафовалаи L2 

Обучим модель уравновешивать маятник и устанавливать конец в начало системы координат таргета
[![Watch the video](https://github.com/Pikudan/PPO/blob/4c8ac915d27fe7b9a70a311848ba6a1ba831d021/video/%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%BE%D0%B2%D0%B5%D1%88%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5.jpg)](https://github.com/Pikudan/PPO/blob/903966790292efba3ba0ebc042a8e169e6d716ec/video/%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%BE%D0%B2%D0%B5%D1%88%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5.mov)
## Репозиторий
```
Контент репозитория
```bash
- main
- video # записи игр
- weight # записи игр
```
Контент файлов (ветка main)
```bash
- main.py
- train.py # train
- test.py # inference 
- envs.py # environment
- agent.py # PPO agent
- network.py # Policy and Value Network
- runners.py # run game in environment
- utils
  - AsArray # transform to ndarray
  - Policy # Policy PPO
  - GAE # Generalized Advantage Estimator
  - TrajectorySampler # samples minibatch
  - NormalizeAdvantages # normalize advantages
  - make_ppo_runner # create runner
  - evaluate # play games on inference
arguments
  - get_args # Arguments parse
```

## Использование

Настройка виртуального окружения
```bash
virtualenv -p python3 venv
source venv/bin/activate
pip install -r requirements.txt
```

Для тренировки:
```bash
mjpython main.py --mode=train --upswing=True --extended_observation=True
```

Для тестирования:
```bash
mjpython main.py --mode=test --upswing=True --extended_observation=True --policy_model=<policy.pth> --value_model=<value.pth>
```

Для исследования связки двух моделей:
```bash
mjpython main.py --mode=research--policy_model=<policy.pth> --value_model=<value.pth>
```

Если флаг upswing поднят, то игра начинается с нижней точки. Если опущен - с верхней точки.

Если флаг extended_observation поднят, то добавляется таргет. 
