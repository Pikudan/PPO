# PPO

##Постановка задачи
С помощью алгоритма Proximal Policy Optimization нужно обучить политику, решающую две подзадачи

1. Подъем маятника из нижнего положения в верхнее с последующей стабилизацией 
2. Постановка конца маятника в соответствии с заданным положением в мировых координатах(таргета)

##Основные используемые идеи
С имплементацией PPO проблем нет, основная задача состоит в разработке функции награды. Она должна поощрять:
-стабильное положение в верхних точках
-положение около таргета
-раскручивание в нижний точках(приложение силы и набирание скорости)

Для этой и других моделей сделаем максимальное время жизни равное 10 секундам и добавим допустимые границы действия агента [-2, 2]. Агенту не нужно много места и времени для раскачивания и стабилизации, и нет смысла исследовать мир вдоль оси x.

##Функция награды
Начнем с первого поощрения. В похожих окружениях агент получает награду, если удерживает объект в пределах 0.2 радиан(угол с горизонталью). Используем самое простое - косинус.

Второе поощрение, или лучше сказать штраф, представляет из себя норму расстояния от центра текущей системы координат(таргета или начало координат).

```python
reward = np.cos(ob[1]) - np.clip((ob[0] - position_ball[0])**2, 0.0, 1.0)
if abs(ob[1]) > 0.2:
  reward -= 2
```
Обучим модель уравновешивать маятник и устанавливать конец в начало системы координат таргета
[![Watch the video](https://github.com/Pikudan/PPO/blob/4c8ac915d27fe7b9a70a311848ba6a1ba831d021/video/%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%BE%D0%B2%D0%B5%D1%88%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5.jpg)](https://github.com/Pikudan/PPO/blob/903966790292efba3ba0ebc042a8e169e6d716ec/video/%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%BE%D0%B2%D0%B5%D1%88%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5.mov)

Можно заметить что агент не стремиться попасть прямо в таргет, а останавливается около него. Можно добавить в окрестности таргета побольше награды. Но оставим это как возможную модификацию.

Для чего начали с этой модели? Для того чтобы определить подходящее поведение награды и при случае менять агента, который поднимает маятник, для большей стабильности.

Третье поощрение:
```python
reward = np.cos(ob[1]) - np.clip((ob[0] - position_ball[0])**2, 0.0, 1.0)
if abs(ob[1]) > 0.2:
  reward -= 2
```
