# PPO

## Постановка задачи
С помощью алгоритма Proximal Policy Optimization нужно обучить политику, решающую две подзадачи

1. Подъем маятника из нижнего положения в верхнее с последующей стабилизацией 
2. Постановка конца маятника в соответствии с заданным положением в мировых координатах(таргета)

## Основные используемые идеи
С имплементацией PPO проблем нет, основная задача состоит в разработке функции награды. Она должна влиять на:
-стабильное положение в верхних точках
-положение около таргета
-раскручивание в нижний точках(приложение силы и набирание скорости)
-стабилизация при раскручивании в верхней точке

Для этой и других моделей установим максимальное время жизни и добавим допустимые границы действия агента [-2, 2]. Агенту не нужно много места и времени для раскачивания и стабилизации, и нет смысла исследовать мир вдоль оси x.

## Функция награды
Начнем с первого пункта. В похожих окружениях агент получает награду, если удерживает объект в пределах 0.2 радиан(угол с горизонталью). Используем самое простое - косинус.

Во втором, cкорее не поощрение, а штраф - минус норма расстояния от центра текущей системы координат(таргета или начало координат).

```python
x = x - target
reward = np.cos(theta) - np.clip((x)**2, 0.0, 1.0)
if abs(theta) > 0.2:
  reward -= 2
```
Обучим модель уравновешивать маятник и устанавливать конец в начало системы координат таргета
[![Watch the video](https://github.com/Pikudan/PPO/blob/4c8ac915d27fe7b9a70a311848ba6a1ba831d021/video/%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%BE%D0%B2%D0%B5%D1%88%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5.jpg)](https://github.com/Pikudan/PPO/blob/903966790292efba3ba0ebc042a8e169e6d716ec/video/%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%BE%D0%B2%D0%B5%D1%88%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5.mov)



Для чего начали с этой модели? Для того чтобы определить подходящее поведение награды и диапозон штрафа, награды(может происходить ситуация, когда падение конца маятника больше уменьшает расстояние чем малое передвижение тележки).

Можно также заметить что агент не стремиться попасть прямо в таргет, а останавливается около него. Можно добавить в окрестности таргета побольше награды, так и поступим далее.

Третий пункт немного сложнее. МЫ должны поощрять скорость по направлению вверх, но тогда есть субоптимальная стратегия -  это бесконечно крутиться. C последним будем бороться тем, что при прохождении полного круга суммарная награда дает отрицательную величину. Проще всего будет наказывать за движение вниз, а на движение вверх не реагировать. Также чтобы агент начал пробовать двигаться добавим награду за работу мотора.

В четвертом пункте будем поощрять за маленькую скорость маятника
```python
x = x - target
if abs(theta) < 0.2:
    reward = np.cos(theta) + (np.exp(1 - np.abs(ob[0])) + 1) * (abs(dtheta) < 1.0) - max(dtheta * theta, 0.0)
elif abs(ob[1]) < np.pi / 2:
    reward = np.cos(theta) * (1 - dtheta**2) - max(dtheta * theta, 0.0)
else:
    reward = - max(dtheta * theta, 0.0) + 0.1 * a**2 - 0.9 # максимум награды в нижней части равен нулю,  -3 < a < 3

```
##Эксперименты
