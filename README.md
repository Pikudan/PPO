# Proximal Policy Optimization

## Постановка задачи
С помощью алгоритма Proximal Policy Optimization нужно обучить политику, решающую две подзадачи

1. Подъем маятника из нижнего положения в верхнее с последующей стабилизацией 
2. Постановка конца маятника в соответствии с заданным положением в мировых координатах - далее **таргет**

## Основные используемые идеи
С имплементацией PPO проблем нет, основная задача состоит в разработке функции награды. Она должна влиять на:  

- размещение конца маятника около таргета  
- раскручивание в нижний точках(приложение силы и достижение нужной скорости для подъема)  
- стабилизация при раскручивании в верхней точке

Следующий момент состоит в том, как можно определить состояние.

Первый вариант:

- положение тележки - $x$ м
- угол маятника с вертикалью - $\theta$ радиан
- скорость тележки - $\frac{dx}{dt}$ м/с
- угловая скорость конца маятника - $\frac{d\theta}{dt}$ рад/с
- положение таргета - $target$ м
  
Второй вариант:

- положение тележки относительно таргета - $х - target$ м
- угол маятника с вертикалью - $\theta$ радиан
- угловая скорость тележки - $\frac{dx}{dt}$ м/с
- скорость конца маятника - $\frac{d\theta}{dt}$ рад/с
  
Для этой и других моделей установим максимальное время жизни. Для агентов, чья задача ограничивается размещением около таргета, добавим допустимые углы маятника с вертикалью в 0.2 радиана. Агенту не нужно много времени для раскачивания и стабилизации, и нет смысла исследовать мир вдоль оси x.

### Функция награды
Начнем с первого пункта. В похожих окружениях агент получает награду, если удерживает объект в пределах 0.2 радиан(угол с вертикалью). Используем самое простое - косинус. Для размещения около таргета используем поощрение -  функцию от нормы расстояния от центра текущей системы координат(таргета или начала координат).



```python
pos_ball = x - target + np.sin(theta) * len_pose
reward = np.cos(theta) - np.exp(1 - (x - target)**2) * (np.abs(ob[0] - ob[4]) < 0.1)
```

```python
reward = np.cos(theta) - np.exp(1 - np.abs(x - target)) * (np.abs(ob[0] - ob[4]) < 0.1)
```

Для чего начали с этой модели? Для того чтобы:

-  определить подходящее поведение награды в пределах 0.2 радиан
-  выбрать L1 или L2 норму
-  сравнить поведение модели  на двух вариантах определения состояния
-  настроить гиперпараметры (количество эпох, $\lambda$)



Второй пункт немного сложнее. Мы должны поощрять скорость по направлению вверх, но тогда есть субоптимальная стратегия -  это бесконечно крутиться. C последним будем бороться тем, что при прохождении нижнего части круга суммарная награда дает неположительную величину. Проще всего будет наказывать за движение вниз, а на движение вверх не реагировать. Также чтобы агент начал пробовать двигаться добавим награду за работу мотора.
```python
reward = min(-theta**2 + 0.1 * dtheta**2 + 0.001 * a**2, 0.0)
```
В третьем пункте будем наказывать за падение в верхней части и поощрять за возвышение
```python
reward = np.cos(theta) - max(dtheta * theta, 0.0)
```

Итоговый вариант функции награды после проведения экспериментов выглядит так:

## Эксперименты

В начале обучались агенты для уравновешивания маятника и устанавливания конца маятника в начало системы координат. Вариант с пространством наблюдений размерности 5 давал стабильнее обучение чем с пространством наблюдений размерности 5, но сходился дольше. Количество эпох из этих экспериментов нужно было брать небольшое - 8-32. 

На видео можно сравнить поведение моделей с
[L1](https://github.com/Pikudan/PPO/blob/51f794d113a53735c0851f2a9985dbfdd922debc/video/%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%BE%D0%B2%D0%B5%D1%88%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_L1.mov)
и
[L2](https://github.com/Pikudan/PPO/blob/51f794d113a53735c0851f2a9985dbfdd922debc/video/%D1%83%D1%80%D0%B0%D0%B2%D0%BD%D0%BE%D0%B2%D0%B5%D1%88%D0%B8%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_L2.mov)
 нормой. Можно также заметить что агент не стремиться попасть прямо в таргет, а останавливается около него. $L_1$ помогает лучше помогает достигать лучших результатов.


 Далее обучение модели стартовало уже с нижней точки.
## Репозиторий
```
Контент репозитория
```bash
- main
- video # записи игр
- weight # записи игр
```
Контент файлов (ветка main)
```bash
- main.py
- train.py # train
- test.py # inference 
- envs.py # environment
- agent.py # PPO agent
- network.py # Policy and Value Network
- runners.py # run game in environment
- utils
  - AsArray # transform to ndarray
  - Policy # Policy PPO
  - GAE # Generalized Advantage Estimator
  - TrajectorySampler # samples minibatch
  - NormalizeAdvantages # normalize advantages
  - make_ppo_runner # create runner
  - evaluate # play games on inference
arguments
  - get_args # Arguments parse
```

## Использование

Настройка виртуального окружения
```bash
virtualenv -p python3 venv
source venv/bin/activate
pip install -r requirements.txt
```

Для тренировки:
```bash
mjpython main.py --mode=train --upswing=True --extended_observation=True
```

Для тестирования:
```bash
mjpython main.py --mode=test --upswing=True --extended_observation=True --policy_model=<policy.pth> --value_model=<value.pth>
```

Для исследования связки двух моделей:
```bash
mjpython main.py --mode=research--policy_model=<policy.pth> --value_model=<value.pth>
```

Если флаг upswing поднят, то игра начинается с нижней точки. Если опущен - с верхней точки.

Если флаг extended_observation поднят, то добавляется таргет. 
